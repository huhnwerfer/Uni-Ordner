{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6855c452",
   "metadata": {},
   "source": [
    "# Session 4: Burrow's Delta\n",
    "\n",
    "In this session, we’ll perform a stylometric analysis on historical text data. We’ll use Burrow's Delta to build a classification model that attributes literary works to their respective authors based on unique writing styles.\n",
    "\n",
    "**To-Do**  \n",
    "Before the session, make sure all of the following packages are installed by running the cell below while using your Anaconda environment as the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c66e7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: plotly in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (6.0.1)\n",
      "Requirement already satisfied: nbformat in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (5.10.4)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from plotly) (1.38.2)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/homebrew/anaconda3/envs/stats_sess_4/lib/python3.12/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.13.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn plotly nbformat matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b2e2e",
   "metadata": {},
   "source": [
    "## 1. Step: Preparing the data\n",
    "\n",
    "We’ll be conducting the stylometric analysis using classic German literature, including works by well-known authors such as Goethe, Schiller, and others.\n",
    "\n",
    "The following CSV file, provided via Moodle, contains 553 German belletristic texts sourced from the Deutsches Textarchiv (DTA).\n",
    "\n",
    "> The DTA offers cross-disciplinary and cross-genre collections and corpora of German-language texts. Its core corpus of around 1,500 titles serves as the foundation for a reference corpus of Modern High German.  \n",
    "> https://www.deutschestextarchiv.de\n",
    "\n",
    "Each document was originally downloaded as an individual XML file from [https://www.deutschestextarchiv.de/download](https://www.deutschestextarchiv.de/download). The XML files were parsed and cleaned of archaic German characters. The CSV contains the processed texts we’ll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910ec6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495e61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba17761",
   "metadata": {},
   "source": [
    "As we know, Burrow's Delta works by comparing a given text to others. To calculate similarities and attribute authorship, we need reference material for each author.  \n",
    "This approach doesn't make sense for authors who only appear once in the dataset, so we need to filter out those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392de80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62780dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d724924e",
   "metadata": {},
   "source": [
    "We need to split three elements from the csv and store them as lists. The texts, in the following name documents, the surnames named authors and the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881b883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0c208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af6360f5",
   "metadata": {},
   "source": [
    "## 2. Step: Extracting Word Frequencies\n",
    "\n",
    "As you know, Burrow's Delta works by comparing distributions of word frequencies. It does so by focusing on so-called function words.\n",
    "\n",
    "**Function words**  \n",
    "> This linguistic category can broadly be defined as the small set of (typically short) words in a language (prepositions, particles, determiners, etc.) which are heavily grammaticalized and which, as opposed to nouns or verbs, often only carry little meaning in isolation (e.g., the versus cat). https://www.humanitiesdataanalysis.org/stylometry/notebook.html\n",
    "\n",
    "We will now create what are called count vectorizations. We use the top 30 tokens. If you have a specific list of words you want to build the model with, it's even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb1b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5357b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b778ec9",
   "metadata": {},
   "source": [
    "Recall that Burrows's Delta assumes word counts to be normalized. Normalization, in this context, means dividing each document vector by its length, where length is measured using a vector norm such as the sum of the components (the L1 norm) or the Euclidean length (L2). L1 normalization is a fancy way of saying that the absolute word frequencies in each document vector will be turned into relative frequencies, through dividing them by their sum (i.e., the total word count of the document, or the sum of the document vector). Scikit-learn’s preprocessing functions specify a number of normalization functions and classes. We use the function normalize() to turn the absolute frequencies into relative frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6a17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26fae914",
   "metadata": {},
   "source": [
    "## Step 3: Splitting Training and Test Data\n",
    "\n",
    "Burrow's Delta doesn't involve traditional training. However, since we want to evaluate how well the method attributes texts to their authors, we need to split the dataset into two parts: one where the authors are known (training data), and one where we’ll attempt to assign authors (test data).  \n",
    "Because we also know the true authors of the test data, we can later evaluate how accurately the method performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11050a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e889000a",
   "metadata": {},
   "source": [
    "## Step 4: Scaling the Vectors\n",
    "\n",
    "In the code block below, we transform the relative word frequencies into *z-scores* using scikit-learn’s `StandardScaler` class.\n",
    "\n",
    "**What are z-scores?**  \n",
    "Z-scores are standardized values that indicate how many standard deviations a data point is from the mean of its distribution. This scaling allows us to compare features on a common scale, which is crucial for distance-based methods like Burrow's Delta.\n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "z_i = \\frac{x_i - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "**Legend:**  \n",
    "- \\(x_i\\): the original value (e.g., word frequency)  \n",
    "- \\( \\mu \\): the mean of the feature across all documents  \n",
    "- \\( \\sigma \\): the standard deviation of the feature  \n",
    "- \\( z_i \\): the resulting z-score  \n",
    "\n",
    "Using z-scores helps normalize the influence of frequent and rare function words across different authors' texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53cb0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1851de18",
   "metadata": {},
   "source": [
    "## Step 5: Calculating the distances\n",
    "\n",
    "In this step we are calculating the distance of all test documents to all training documents.\n",
    "\n",
    "Cityblock (Manhatten) distance: \n",
    "$$\n",
    "D(x, y) = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e914ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8fec97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb3ddd09",
   "metadata": {},
   "source": [
    "## Step 5: Evaluating the Attribution\n",
    "\n",
    "After applying Burrow's Delta to assign authors to the test texts, we can evaluate how well the method performed.\n",
    "\n",
    "One common metric is **accuracy**, which tells us the proportion of correctly attributed texts:\n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$\n",
    "\n",
    "While accuracy gives a general idea of performance, it doesn't show which authors were confused with each other. For that, we use a **confusion matrix**.\n",
    "\n",
    "**Confusion Matrix:**  \n",
    "A confusion matrix is a table that shows the counts of actual vs. predicted author labels. Each row represents the true author, while each column represents the predicted author. Ideally, most values should fall along the diagonal, indicating correct attributions.\n",
    "\n",
    "This helps us analyze:\n",
    "- Which authors are often misclassified\n",
    "- Whether the model struggles more with certain authors than others\n",
    "\n",
    "We’ll now compute both metrics to assess the results of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5207f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abbb48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stats_sess_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
